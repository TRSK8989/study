{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各種モジュールのimport\n",
    "\n",
    "・このプログラムで使うモジュールです。  \n",
    "・tqdmモジュールについては、notebook形式ならtqdm_notebook、通常のpythonファイルならtqdmでしか動かないみたいです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "#from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNetクラスの定義\n",
    "\n",
    "・UNetの構造ををBlock単位で細分化しています(UNetDownBlockクラス、UNetUpBlockクラス)。  \n",
    "・Pythonのクラス定義の勉強も兼ねています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#エンコーダ部分の構成単位\n",
    "class UNetDownBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, down_mode=\"max_pooling\", kernel_size=None, stride=None, padding=None):\n",
    "        super(UNetDownBlock, self).__init__()\n",
    "\n",
    "        self.down_mode = down_mode\n",
    "        \n",
    "        #1/2倍縮小：プーリングを使う場合\n",
    "        if self.down_mode == \"max_pooling\":\n",
    "            self.mp = nn.MaxPool2d(2, 2)\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        #1/2倍縮小：畳み込みを使う場合\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        \n",
    "        #縮小以外のBlockの構成要素\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #down_modeがプーリング以外であればここで縮小\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        #down_modeがプーリングであればここで縮小\n",
    "        if self.down_mode == \"max_pooling\":\n",
    "            x = self.mp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "#デコーダ部分の構成単位\n",
    "class UNetUpBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, up_mode = \"interpolation\"):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "\n",
    "        #2倍拡大：補間法を使う場合\n",
    "        if up_mode == \"interpolation\":\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "            \n",
    "        #2倍拡大：転置畳み込み層を使う場合\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        #拡大以外のブロックの構成要素\n",
    "        self.conv1 = nn.Conv2d(in_channels=out_channels*2, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    #ブロック単位の処理の流れ\n",
    "    def forward(self, x, x_fromE):\n",
    "        \n",
    "        #拡大\n",
    "        x = self.up(x)\n",
    "        #特徴マップの連結\n",
    "        x = torch.cat((x, x_fromE), 1)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "\n",
    "#UNetの本体\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=None, filter_num=16, up_mode=\"transpose\", down_mode=\"max_pooling\"):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        #エンコーダ部分の構成要素\n",
    "        #self.down1ブロックは縮小しない\n",
    "        self.down1 = UNetDownBlock(n_channels, filter_num, down_mode=\"conv\", kernel_size=3, stride=1, padding=1)\n",
    "        self.down2 = UNetDownBlock(filter_num, filter_num*2, down_mode=down_mode)\n",
    "        self.down3 = UNetDownBlock(filter_num*2, filter_num*4, down_mode=down_mode)\n",
    "        self.down4 = UNetDownBlock(filter_num*4, filter_num*4, down_mode=down_mode)\n",
    "        #self.down5 = UNetDownBlock(filter_num*8, filter_num*8, down_mode=down_mode)\n",
    "\n",
    "        #デコーダ部分の構成要素\n",
    "        #self.up1 = UNetUpBlock(filter_num*8, filter_num*8, up_mode=up_mode)\n",
    "        self.up2 = UNetUpBlock(filter_num*4, filter_num*4, up_mode=up_mode)\n",
    "        self.up3 = UNetUpBlock(filter_num*4, filter_num*2, up_mode=up_mode)\n",
    "        self.up4 = UNetUpBlock(filter_num*2, filter_num, up_mode=up_mode)\n",
    "\n",
    "        self.conv_final = nn.Sequential(nn.Conv2d(filter_num, n_classes, 1, 1, 0))\n",
    "        \n",
    "        \n",
    "\n",
    "    #UNetの処理の流れ\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #エンコーダ部分の処理の流れ\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        #x5 = self.down5(x4)\n",
    "\n",
    "        #デコーダ部分の処理の流れ\n",
    "        #x = self.up1(x5, x4)\n",
    "        x = self.up2(x4, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.conv_final(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自前データセットクラスの定義\n",
    "\n",
    "・自前のデータセットの読み込み方に関する部分です。  \n",
    "・Data augmentationはFlipとrotateの2種類しか使っていませんが、他にも使いたいものがあれば追加してもらって大丈夫です。  \n",
    "・low_resource_cropping関数は画像を切り取って入力することで学習時のメモリ不足を解消します。必要なければコメントアウトしてください。  \n",
    "・教師ラベルの値はひび割れを1、背景を0としています。(torchのラベルはなぜかlong型にしないといけないらしい...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrackDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mode=\"train\", data_source_dir=None, img_preprocessing=None):\n",
    "        \n",
    "        self.mode = mode\n",
    "        \n",
    "        self.image_dir = sorted(glob.glob(os.path.join(data_source_dir, mode, \"img\", \"*.jpg\")))\n",
    "        self.label_dir = sorted(glob.glob(os.path.join(data_source_dir, mode, \"label\", \"*.jpg\")))\n",
    "        \n",
    "        self.img_preprocessing = img_preprocessing\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_dir)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #データセットの読み込み処理の流れ\n",
    "    def __getitem__(self , index):\n",
    "        img_path = self.image_dir[index]\n",
    "        label_path = self.label_dir[index]\n",
    "        \n",
    "        #グレースケール画像として読み込み\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        label = Image.open(label_path).convert(\"L\")\n",
    "        \n",
    "        #trainモードのみ、DataAugmentationとCroppingを行う\n",
    "        if self.mode == \"train\":\n",
    "            img, label = data_augmentation(img, label)\n",
    "            img, label = low_resource_cropping(img, label)\n",
    "        \n",
    "        #testモードの場合、ネットワークの入力に適したサイズに変換する必要がある\n",
    "        elif self.mode == \"test\":\n",
    "            img, label = resize_test_img(img, label)\n",
    "            \n",
    "        #前処理によってtorch.tensorに変換\n",
    "        img = self.img_preprocessing(img)\n",
    "        label = label_preprocessing(label)\n",
    "        \n",
    "        return img, label\n",
    "        \n",
    "\n",
    "        \n",
    "#学習用画像のData Augmentation\n",
    "def data_augmentation(input_img, input_label, low_resource_ver=False):\n",
    "    \n",
    "    #random horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        input_img = input_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        input_label = input_label.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "    #random rotation\n",
    "    r = random.random()\n",
    "    if r > 0.75:\n",
    "        input_img = input_img.rotate(270)\n",
    "        input_label = input_label.rotate(270)\n",
    "    elif r > 0.5:\n",
    "        input_img = input_img.rotate(180)\n",
    "        input_label = input_label.rotate(180)\n",
    "    elif r > 0.25:\n",
    "        input_img = input_img.rotate(90)\n",
    "        input_label = input_label.rotate(90)\n",
    "        \n",
    "    return input_img, input_label\n",
    "\n",
    "\n",
    "        \n",
    "#GPUのリソースが少ない人向けのCropping(256×256 → 192×192)\n",
    "#4GB程度の人は下記関数を使うことをオススメする\n",
    "def low_resource_cropping(input_img, input_label):\n",
    "    \n",
    "    crop_size=192\n",
    "    w, h = input_img.size\n",
    "    x1 = random.randint(0, w - crop_size)\n",
    "    y1 = random.randint(0, h - crop_size)\n",
    "    input_img = input_img.crop((x1, y1, x1 + crop_size, y1 + crop_size))\n",
    "    input_label = input_label.crop((x1, y1, x1 + crop_size, y1 + crop_size))\n",
    "\n",
    "    return input_img, input_label\n",
    "\n",
    "\n",
    "\n",
    "#教師ラベルの前処理\n",
    "def label_preprocessing(label):\n",
    "    \n",
    "    label = np.where(np.array(label) > 127, 1, 0)\n",
    "    label = torch.from_numpy(label).to(torch.long)\n",
    "    \n",
    "    return label\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習時の各種パラメータ設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GPUの認識\n",
    "device = torch.device(\"cuda:0\")\n",
    "#入力画像のチャネル数\n",
    "n_channels = 1\n",
    "#クラス数(今回はひび割れ or 背景の2クラス)\n",
    "n_classes = 2\n",
    "#UNetのフィルタ数の基本単位\n",
    "filter_num = 16\n",
    "#オプティマイザの学習率\n",
    "lr = 5e-4\n",
    "#学習時のバッチサイズ\n",
    "batch_size = 16\n",
    "#入力画像の前処理\n",
    "img_preprocessing = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "#データセットが存在するディレクトリのパス指定\n",
    "data_source_dir = \"./Crack/dataset\"\n",
    "#学習エポック数\n",
    "num_epochs = 50\n",
    "#最良ロスの定義(初期値はinf)\n",
    "best_loss = float(\"inf\")\n",
    "#実験結果のディレクトリのパス指定\n",
    "result_dir = \"./Crack/result\"\n",
    "\n",
    "\n",
    "\n",
    "#UNetの定義\n",
    "model = UNet(n_channels=n_channels, n_classes=n_classes, filter_num=filter_num, up_mode=\"interpolation\", down_mode=\"max_pooling\")\n",
    "#UNetのパラメータをGPUへ\n",
    "model = model.to(device)\n",
    "#損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "#オプティマイザの定義\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#スケジューラの定義\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=3, verbose=True)\n",
    "#ネットワーク構成の確認\n",
    "summary(model, (1,192,192))\n",
    "\n",
    "\n",
    "#データセットの定義\n",
    "train_dataset = CrackDataset(mode=\"train\", data_source_dir=data_source_dir, img_preprocessing=img_preprocessing)\n",
    "validation_dataset = CrackDataset(mode=\"validation\", data_source_dir=data_source_dir, img_preprocessing=img_preprocessing)\n",
    "\n",
    "#データローダーの定義\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習時の処理の流れ\n",
    "\n",
    "・プログレスバーをtqdmで表示している。もしtqdmを使わないなら、tbarの部分をdataloaderに置き換える。  \n",
    "・損失が最小になったタイミングのみモデルを保存する。best_lossの初期値はinfなので1エポック目は絶対に保存される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(f\"Starting epoch: {epoch}\")\n",
    "    \n",
    "    losses = 0.0\n",
    "    total_batches = len(train_dataloader)\n",
    "    #UNet(特にbatch_norm)をtrainモードへ\n",
    "    model.train()\n",
    "    #プログレスバー表示準備\n",
    "    tbar = tqdm(train_dataloader)\n",
    "    #オプティマイザの勾配情報を初期化\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for itr, batch in enumerate(tbar):\n",
    "        \n",
    "        #データローダーの出力タプルを入力画像とラベルに分割\n",
    "        img, label = batch\n",
    "        #入力画像をGPUへ\n",
    "        img = img.to(device)\n",
    "        #ラベルをGPUへ\n",
    "        label = label.to(device)\n",
    "        #入力画像をUNetに流して出力を得る(順伝搬)\n",
    "        output = model(img)\n",
    "        #出力とラベルから損失を計算\n",
    "        loss = criterion(output, label)\n",
    "        #損失から勾配情報を算出(逆伝搬)\n",
    "        loss.backward()\n",
    "        #勾配情報からUNetの各層の重みとバイアスを更新\n",
    "        optimizer.step()\n",
    "        #オプティマイザの勾配情報を初期化\n",
    "        optimizer.zero_grad()\n",
    "        #処理済みのバッチの累計損失を計算\n",
    "        losses += loss.item()\n",
    "        #バッチ処理中の平均損失を表示\n",
    "        tbar.set_description('loss: %.7f' % (losses / (itr + 1)))\n",
    "        \n",
    "    #エポック終了時の平均損失の算出\n",
    "    train_epoch_loss = losses / total_batches\n",
    "        \n",
    "    \n",
    "    #勾配情報の算出を省略\n",
    "    with torch.no_grad():\n",
    "        losses = 0.0\n",
    "        total_batches = len(validation_dataloader)\n",
    "        #UNet(特にbatch_norm)をevalモードへ\n",
    "        model.eval()\n",
    "        tbar = tqdm(validation_dataloader)\n",
    "\n",
    "        for itr, batch in enumerate(tbar):\n",
    "\n",
    "            img, label = batch\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "            losses += loss.item()\n",
    "            tbar.set_description('loss: %.7f' % (losses / (itr + 1)))\n",
    "                \n",
    "        validation_epoch_loss = losses / total_batches\n",
    "        #スケジューラの更新\n",
    "        scheduler.step(validation_epoch_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #CSVファイルへログの書き出し\n",
    "    with open((os.path.join(result_dir, 'training_log.csv')), 'a') as f:\n",
    "        \n",
    "        writer = csv.writer(f)\n",
    "        if epoch == 0:\n",
    "            writer.writerow([\"epoch\", \"train_loss\", \"validation_loss\"])\n",
    "        writer.writerow([epoch+1, train_epoch_loss, validation_epoch_loss])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #ロスが最小のときモデルのパラメータを保存する\n",
    "    if validation_epoch_loss < best_loss:\n",
    "        \n",
    "        best_loss = validation_epoch_loss\n",
    "        state = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"best_loss\": best_loss,\n",
    "        }\n",
    "        filename = os.path.join(result_dir, \"checkpoint.pth.tar\")\n",
    "        torch.save(state, filename)\n",
    "        print(\"----------new optimal found! saving state---------- \")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テスト時のみ使う関数の定義\n",
    "\n",
    "・テスト画像はサイズがバラバラなため、resize_test_imgでUNetに入力できるサイズに変形する。  \n",
    "・ひび割れのクラスを対象にして適合率と再現率を計算する。他の指標も使いたい場合は各自追加してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_test_img(img, label):\n",
    "    \n",
    "    img_size = 192\n",
    "    \n",
    "    for w in range(1, 100):\n",
    "        if w * img_size >= img.size[0]:\n",
    "            width = w * img_size\n",
    "            break\n",
    "    for h in range(1, 100):\n",
    "        if h * img_size >= img.size[1]:\n",
    "            height = h * img_size\n",
    "            break\n",
    "            \n",
    "    img = img.resize((width, height))\n",
    "    label = label.resize((width, height))\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "\n",
    "\n",
    "def softmax_numpy(output_row):\n",
    "    output_prob = np.exp(output_row)\n",
    "    op_sum = np.sum(output_prob, axis=0)\n",
    "    return output_prob / op_sum\n",
    "\n",
    "\n",
    "\n",
    "def save_img(output, label, directory, itr):\n",
    "    \n",
    "    save_dir = os.path.join(result_dir, \"maxprob\")\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    cv2.imwrite(os.path.join(save_dir, str(itr) + \".jpg\"), output*255)\n",
    "    \n",
    "    \n",
    "\n",
    "def get_score(output, label, result_dir, itr):\n",
    "    \n",
    "    #多クラスの場合\n",
    "    \"\"\"\n",
    "    tp = torch.empty(n_classes)\n",
    "    fp = torch.empty(n_classes)\n",
    "    fn = torch.empty(n_classes)\n",
    "    tn = torch.empty(n_classes)\n",
    "    print(output.shape)\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        tp[i] = ((output == i) & (label == i)).sum().item()\n",
    "        fp[i] = ((output == i) & (label != i)).sum().item()\n",
    "        fn[i] = ((output != i) & (label == i)).sum().item()\n",
    "        tn[i] = ((output != i) & (label != i)).sum().item()\n",
    "    \n",
    "    pre = tp.sum() / (tp + fp).sum()\n",
    "    rec = tp.sum() / (tp + fn).sum()\n",
    "    \"\"\"\n",
    "\n",
    "    #単一クラスの場合(label=1: ひび割れ)\n",
    "    detected_label = 1\n",
    "    tp = ((output == detected_label) & (label == detected_label)).sum().item()\n",
    "    fp = ((output == detected_label) & (label != detected_label)).sum().item()\n",
    "    fn = ((output != detected_label) & (label == detected_label)).sum().item()\n",
    "    tn = ((output != detected_label) & (label != detected_label)).sum().item()\n",
    "    precision = 100 * tp / (tp + fp)\n",
    "    recall = 100 * tp / (tp + fn)\n",
    "    \n",
    "    with open((os.path.join(result_dir, 'eval.csv')), 'a') as f:\n",
    "        \n",
    "        writer = csv.writer(f)\n",
    "        if itr == 0:\n",
    "            writer.writerow([\"image_number\", \"precision\", \"recall\"])\n",
    "        writer.writerow([str(itr) + \".jpg\", precision, recall])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テスト時の各種パラメータ設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "n_channels = 1\n",
    "n_classes = 2\n",
    "filter_num = 16\n",
    "img_preprocessing = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "data_source_dir = \"./Crack/dataset\"\n",
    "result_dir = \"./Crack/result\"\n",
    "check_path = result_dir + \"/checkpoint.pth.tar\"\n",
    "\n",
    "model = UNet(n_channels=n_channels, n_classes=n_classes, filter_num=filter_num, up_mode=\"interpolation\")\n",
    "model = model.to(device)\n",
    "checkpoint = torch.load(check_path)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "test_dataset = CrackDataset(mode=\"test\", data_source_dir=data_source_dir, img_preprocessing=img_preprocessing)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テスト時の処理の流れ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tbar = tqdm(test_dataloader)\n",
    "model.eval()\n",
    "for itr, batch in enumerate(tbar):\n",
    "    with torch.no_grad():\n",
    "        img, label = batch\n",
    "        img = img.to(device)\n",
    "        output = model(img)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        #不必要な軸の削減\n",
    "        output = np.squeeze(output, 0)\n",
    "        #出力をsoftmaxで確率に変換\n",
    "        output = softmax_numpy(output)\n",
    "        #出力確率の最大値を算出\n",
    "        output = np.argmax(output, axis=0)\n",
    "        label = label.cpu().numpy()\n",
    "        \n",
    "        get_score(output, label, result_dir, itr)\n",
    "        save_img(output, label, result_dir, itr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
